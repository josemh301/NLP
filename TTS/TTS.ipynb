{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Idea: Text-to-Phoneme Conversion\n",
    "\n",
    "**Objective**: A critical step in many TTS systems is converting the text into phonemes (distinct units of sound). This project involve building a model that takes standard English text and predicts its phonemic transcription in X-SAMPA format.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "    - Split CSV file into training, validation, and test sets.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "    - Character tokenization the English text\n",
    "    - Tokenize the X-SAMPA transcriptions. Here, each unique X-SAMPA symbol can be a token.\n",
    "    - Convert these tokens to integer representations.\n",
    "\n",
    "3. **Modeling**:\n",
    "    - Sequence-to-sequence models: LSTM\n",
    "    - English text input tokenization\n",
    "\n",
    "4. **Training & Evaluation**:\n",
    "    - Train your model on the training set, evaluate on the validation set, and fine-tune accordingly.\n",
    "    - Once satisfied, evaluate its performance on the test set.\n",
    "\n",
    "5. **Inference**:\n",
    "    - Build a function where you input standard English text, and it outputs the predicted X-SAMPA transcription.\n",
    "\n",
    "**Technologies**:\n",
    "- Python\n",
    "    - ``csv``\n",
    "    - ``keras``\n",
    "    - ``sklearn``\n",
    "- X-Sampa\n",
    "- librosa (for audio processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Transcription                     tokenized_x_sampa\n",
      "34  fAr h@\"raIz@n  [f, A, r, h, @, \", r, a, I, z, @, n]\n",
      "35  fA: h@\"raIz@n  [f, A, :, h, @, \", r, a, I, z, @, n]\n",
      "92  Endl@s oUS@nz  [E, n, d, l, @, s, o, U, S, @, n, z]\n",
      "93  EndlIs oUS@nz  [E, n, d, l, I, s, o, U, S, @, n, z]\n",
      "98   glIs.nIN du:     [g, l, I, s, ., n, I, N, d, u, :]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data\n",
    "data = pd.read_csv('transcription.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Group by WordID\n",
    "grouped = list(data.groupby('WordID'))\n",
    "\n",
    "# Split the grouped data\n",
    "train, temp = train_test_split(grouped, test_size=0.3, random_state=42)  \n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=42)  \n",
    "\n",
    "# Extracting the dataframes from the groups\n",
    "train_df = pd.concat([item[1] for item in train])\n",
    "valid_df = pd.concat([item[1] for item in valid])\n",
    "test_df = pd.concat([item[1] for item in test])\n",
    "\n",
    "# X-SAMPA tokenization\n",
    "def tokenize_x_sampa(transcription):\n",
    "    # List of known multi-character X-SAMPA symbols \n",
    "    # This list might need more symbols based on the entire dataset\n",
    "    multi_char_symbols = [\":l\", \"^m\", \"O:\"]\n",
    "    \n",
    "    tokens = []\n",
    "    for symbol in transcription.split():\n",
    "        if symbol in multi_char_symbols:\n",
    "            tokens.append(symbol)\n",
    "        else:\n",
    "            tokens.extend(list(symbol))\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the Transcription column for each dataframe\n",
    "train_df['tokenized_x_sampa'] = train_df['Transcription'].apply(tokenize_x_sampa)\n",
    "valid_df['tokenized_x_sampa'] = valid_df['Transcription'].apply(tokenize_x_sampa)\n",
    "test_df['tokenized_x_sampa'] = test_df['Transcription'].apply(tokenize_x_sampa)\n",
    "\n",
    "# Display the first few rows for verification\n",
    "print(train_df[['Transcription', 'tokenized_x_sampa']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Character tokenization the English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (X-SAMPA symbols): ['\"', '.', '3', ':', '@', '^', 'a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'z', 'รฐ', 'ล']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to convert the list of tokens back to space-separated string\n",
    "def tokens_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Convert the list of tokens in 'tokenized_x_sampa' back to space-separated string\n",
    "train_df['x_sampa_str'] = train_df['tokenized_x_sampa'].apply(tokens_to_string)\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word', token_pattern=r\"(?:\\S+)\")\n",
    "# We're using the token_pattern parameter to make sure we capture the entire X-SAMPA symbols\n",
    "\n",
    "# Apply the vectorizer on the 'x_sampa_str' column\n",
    "X = vectorizer.fit_transform(train_df['x_sampa_str'])\n",
    "\n",
    "# Getting the token names\n",
    "tokens = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Tokens (X-SAMPA symbols):\", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tokenize the X-SAMPA transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Transcription                      tokenized_x_sampa\n",
      "34    fAr h@\"raIz@n   [f, A, r, h, @, \", r, a, I, z, @, n]\n",
      "35    fA: h@\"raIz@n   [f, A, :, h, @, \", r, a, I, z, @, n]\n",
      "92    Endl@s oUS@nz   [E, n, d, l, @, s, o, U, S, @, n, z]\n",
      "93    EndlIs oUS@nz   [E, n, d, l, I, s, o, U, S, @, n, z]\n",
      "98     glIs.nIN du:      [g, l, I, s, ., n, I, N, d, u, :]\n",
      "..              ...                                    ...\n",
      "153  wO:m \"s^nlaIt\"  [w, O:, m, \", s, ^, n, l, a, I, t, \"]\n",
      "70       raIzIN s^n            [r, a, I, z, I, N, s, ^, n]\n",
      "71       raIzIN s^n            [r, a, I, z, I, N, s, ^, n]\n",
      "172   sAft \"pIloUz\"   [s, A, f, t, \", p, I, l, o, U, z, \"]\n",
      "173   sOft \"pIl@Uz\"   [s, O, f, t, \", p, I, l, @, U, z, \"]\n",
      "\n",
      "[160 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of known multi-character X-SAMPA symbols (this is just a sample; there might be more)\n",
    "multi_char_symbols = [\":l\", \"^m\", \"O:\"]\n",
    "# If there are more multi-character symbols in your data, add them to the list\n",
    "\n",
    "def tokenize_x_sampa(transcription):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(transcription):\n",
    "        # Check if the current char and next char form a multi-char symbol\n",
    "        if i < len(transcription) - 1 and transcription[i:i+2] in multi_char_symbols:\n",
    "            tokens.append(transcription[i:i+2])\n",
    "            i += 2\n",
    "        # Check for single-char symbols\n",
    "        elif transcription[i] != ' ':\n",
    "            tokens.append(transcription[i])\n",
    "            i += 1\n",
    "        # If it's just a space, skip to the next character\n",
    "        else:\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the Transcription column of train_df\n",
    "train_df['tokenized_x_sampa'] = train_df['Transcription'].apply(tokenize_x_sampa)\n",
    "\n",
    "# Display the results for train_df\n",
    "print(train_df[['Transcription', 'tokenized_x_sampa']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Convert these tokens to integer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    english_str                                 english_seq  \\\n",
      "34        F a r   h o r i z o n         [19, 8, 5, 11, 10, 5, 2, 23, 10, 4]   \n",
      "35        F a r   h o r i z o n         [19, 8, 5, 11, 10, 5, 2, 23, 10, 4]   \n",
      "92  E n d l e s s   o c e a n s  [3, 4, 12, 6, 3, 1, 1, 10, 16, 3, 8, 4, 1]   \n",
      "93  E n d l e s s   o c e a n s  [3, 4, 12, 6, 3, 1, 1, 10, 16, 3, 8, 4, 1]   \n",
      "98  G l i s t e n i n g   d e w   [7, 6, 2, 1, 9, 3, 4, 2, 4, 7, 12, 3, 13]   \n",
      "\n",
      "                x_sampa_str                            x_sampa_seq  \n",
      "34  f A r h @ \" r a I z @ n         [21, 14, 5, 25, 5, 9, 1, 7, 3]  \n",
      "35  f A : h @ \" r a I z @ n            [21, 14, 25, 5, 9, 1, 7, 3]  \n",
      "92  E n d l @ s o U S @ n z     [26, 3, 8, 4, 2, 24, 12, 18, 3, 7]  \n",
      "93  E n d l I s o U S @ n z  [26, 3, 8, 4, 1, 2, 24, 12, 18, 3, 7]  \n",
      "98    g l I s . n I N d u :         [19, 4, 1, 2, 3, 1, 10, 8, 29]  \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Convert the list of tokens back to space-separated strings for both English and X-SAMPA\n",
    "train_df['english_str'] = train_df['Word'].apply(lambda x: ' '.join(list(x)))  # For English character tokenization\n",
    "train_df['x_sampa_str'] = train_df['tokenized_x_sampa'].apply(' '.join)\n",
    "\n",
    "valid_df['english_str'] = valid_df['Word'].apply(lambda x: ' '.join(list(x)))\n",
    "valid_df['x_sampa_str'] = valid_df['tokenized_x_sampa'].apply(' '.join)\n",
    "\n",
    "test_df['english_str'] = test_df['Word'].apply(lambda x: ' '.join(list(x)))\n",
    "test_df['x_sampa_str'] = test_df['tokenized_x_sampa'].apply(' '.join)\n",
    "\n",
    "# Initialize and fit the tokenizer for English text\n",
    "english_tokenizer = Tokenizer(char_level=False, lower=True, split=' ')\n",
    "english_tokenizer.fit_on_texts(train_df['english_str'])\n",
    "\n",
    "# Convert English tokens to integer sequences\n",
    "train_df['english_seq'] = english_tokenizer.texts_to_sequences(train_df['english_str'])\n",
    "valid_df['english_seq'] = english_tokenizer.texts_to_sequences(valid_df['english_str'])\n",
    "test_df['english_seq'] = english_tokenizer.texts_to_sequences(test_df['english_str'])\n",
    "\n",
    "# Initialize and fit the tokenizer for X-SAMPA transcriptions\n",
    "x_sampa_tokenizer = Tokenizer(char_level=False, lower=False, split=' ')\n",
    "x_sampa_tokenizer.fit_on_texts(train_df['x_sampa_str'])\n",
    "\n",
    "# Convert X-SAMPA tokens to integer sequences\n",
    "train_df['x_sampa_seq'] = x_sampa_tokenizer.texts_to_sequences(train_df['x_sampa_str'])\n",
    "valid_df['x_sampa_seq'] = x_sampa_tokenizer.texts_to_sequences(valid_df['x_sampa_str'])\n",
    "test_df['x_sampa_seq'] = x_sampa_tokenizer.texts_to_sequences(test_df['x_sampa_str'])\n",
    "\n",
    "# Let's check the conversion\n",
    "print(train_df[['english_str', 'english_seq', 'x_sampa_str', 'x_sampa_seq']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    6656        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 256)    9216        input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 512), (None, 1574912     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 512),  1574912     embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 36)     18468       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,184,164\n",
      "Trainable params: 3,184,164\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "\n",
    "# Define parameters\n",
    "english_vocab_size = len(english_tokenizer.word_index) + 1\n",
    "x_sampa_vocab_size = len(x_sampa_tokenizer.word_index) + 1\n",
    "embedding_dim = 256\n",
    "lstm_units = 512\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "encoder_embedding = Embedding(english_vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(lstm_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(x_sampa_vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = Dense(x_sampa_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.2 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Ensure all sequences have the same length\n",
    "max_english_seq_length = max([len(seq) for seq in train_df['Word']])\n",
    "max_x_sampa_seq_length = max([len(seq) for seq in train_df['x_sampa_str']])\n",
    "\n",
    "# Convert English characters to integers\n",
    "english_tokenizer = Tokenizer(char_level=True, oov_token='UNK')\n",
    "english_tokenizer.fit_on_texts(train_df['Word'])\n",
    "english_int_seq = english_tokenizer.texts_to_sequences(train_df['Word'])\n",
    "english_int_seq_padded = pad_sequences(english_int_seq, maxlen=max_english_seq_length, padding='post')\n",
    "\n",
    "# Convert X-SAMPA tokens to integers\n",
    "x_sampa_tokenizer = Tokenizer(oov_token='UNK')\n",
    "x_sampa_tokenizer.fit_on_texts(train_df['x_sampa_str'])\n",
    "x_sampa_int_seq = x_sampa_tokenizer.texts_to_sequences(train_df['x_sampa_str'])\n",
    "x_sampa_int_seq_padded = pad_sequences(x_sampa_int_seq, maxlen=max_x_sampa_seq_length, padding='post')\n",
    "\n",
    "# Preparing decoder input and output. Decoder input is the original sequence, and the output is the sequence shifted by one timestep.\n",
    "decoder_input_data = x_sampa_int_seq_padded[:, :-1]\n",
    "decoder_target_data = x_sampa_int_seq_padded[:, 1:]\n",
    "\n",
    "# The encoder input remains the same\n",
    "encoder_input_data = english_int_seq_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:789 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\losses.py:1666 categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\backend.py:4839 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 34, 27) and (None, 34, 36) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2fd00ed8c4c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mvalid_encoder_input_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_decoder_input_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_decoder_target_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3036\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m-> 3038\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3458\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m   3459\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[1;32m-> 3460\u001b[1;33m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3381\u001b[0m     graph_function = self._create_graph_function(\n\u001b[1;32m-> 3382\u001b[1;33m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[0;32m   3383\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3308\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\training.py:789 train_step\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\losses.py:1666 categorical_crossentropy\n        y_true, y_pred, from_logits=from_logits, axis=axis)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\keras\\backend.py:4839 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    C:\\Users\\josem\\anaconda3\\envs\\mlcc\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 34, 27) and (None, 34, 36) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Convert and pad validation sequences for English\n",
    "valid_english_int_seq = english_tokenizer.texts_to_sequences(valid_df['Word'])\n",
    "valid_english_int_seq_padded = pad_sequences(valid_english_int_seq, maxlen=max_english_seq_length, padding='post')\n",
    "\n",
    "# Convert and pad validation sequences for X-SAMPA\n",
    "valid_x_sampa_int_seq = x_sampa_tokenizer.texts_to_sequences(valid_df['x_sampa_str'])\n",
    "valid_x_sampa_int_seq_padded = pad_sequences(valid_x_sampa_int_seq, maxlen=max_x_sampa_seq_length, padding='post')\n",
    "\n",
    "# Prepare decoder input and output data for the validation set\n",
    "valid_decoder_input_data = valid_x_sampa_int_seq_padded[:, :-1]\n",
    "valid_decoder_target_data = valid_x_sampa_int_seq_padded[:, 1:]\n",
    "\n",
    "# The encoder input for validation remains the same\n",
    "valid_encoder_input_data = valid_english_int_seq_padded\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "\n",
    "# Convert the decoder target data for training set to one-hot encoding for training\n",
    "decoder_target_onehot = np.zeros((len(train_df), max_x_sampa_seq_length-1, len(x_sampa_tokenizer.word_index)+1), dtype='float32')  # Notice the change in dimensions\n",
    "for i, seq in enumerate(decoder_target_data):\n",
    "    for j, token in enumerate(seq):\n",
    "        if token > 0:\n",
    "            decoder_target_onehot[i, j, token] = 1.\n",
    "\n",
    "# Convert the decoder target data for validation set to one-hot encoding for validation\n",
    "valid_decoder_target_onehot = np.zeros((len(valid_df), max_x_sampa_seq_length-1, len(x_sampa_tokenizer.word_index)+1), dtype='float32')  # Notice the change in dimensions\n",
    "for i, seq in enumerate(valid_decoder_target_data):\n",
    "    for j, token in enumerate(seq):\n",
    "        if token > 0:\n",
    "            valid_decoder_target_onehot[i, j, token] = 1.\n",
    "\n",
    "history = model.fit(\n",
    "    [encoder_input_data, decoder_input_data], \n",
    "    decoder_target_onehot,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=([valid_encoder_input_data, valid_decoder_input_data], valid_decoder_target_onehot)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5.1 Split CSV file into training, validation, and test sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
