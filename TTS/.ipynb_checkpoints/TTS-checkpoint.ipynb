{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Idea: Text-to-Phoneme Conversion\n",
    "\n",
    "**Objective**: A critical step in many TTS systems is converting the text into phonemes (distinct units of sound). This project involve building a model that takes standard English text and predicts its phonemic transcription in X-SAMPA format.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "    - Split CSV file into training, validation, and test sets.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "    - Character tokenization the English text\n",
    "    - Tokenize the X-SAMPA transcriptions. Here, each unique X-SAMPA symbol can be a token.\n",
    "    - Convert these tokens to integer representations.\n",
    "\n",
    "3. **Modeling**:\n",
    "    - You can use sequence-to-sequence models, which have been successful for such tasks. LSTM or GRU based architectures would be a good starting point.\n",
    "    - Input is the tokenized English text, and the target is the X-SAMPA transcription.\n",
    "\n",
    "4. **Training & Evaluation**:\n",
    "    - Train your model on the training set, evaluate on the validation set, and fine-tune accordingly.\n",
    "    - Once satisfied, evaluate its performance on the test set.\n",
    "\n",
    "5. **Inference**:\n",
    "    - Build a function where you input standard English text, and it outputs the predicted X-SAMPA transcription.\n",
    "\n",
    "**Technologies**:\n",
    "- Python\n",
    "    - ``csv``\n",
    "    - ``keras``\n",
    "    - ``sklearn``\n",
    "- X-Sampa\n",
    "- librosa (for audio processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Transcription                     tokenized_x_sampa\n",
      "34  fAr h@\"raIz@n  [f, A, r, h, @, \", r, a, I, z, @, n]\n",
      "35  fA: h@\"raIz@n  [f, A, :, h, @, \", r, a, I, z, @, n]\n",
      "92  Endl@s oUS@nz  [E, n, d, l, @, s, o, U, S, @, n, z]\n",
      "93  EndlIs oUS@nz  [E, n, d, l, I, s, o, U, S, @, n, z]\n",
      "98   glIs.nIN du:     [g, l, I, s, ., n, I, N, d, u, :]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV data\n",
    "data = pd.read_csv('transcription.csv')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Group by WordID\n",
    "grouped = list(data.groupby('WordID'))\n",
    "\n",
    "# Split the grouped data\n",
    "train, temp = train_test_split(grouped, test_size=0.3, random_state=42)  \n",
    "valid, test = train_test_split(temp, test_size=0.5, random_state=42)  \n",
    "\n",
    "# Extracting the dataframes from the groups\n",
    "train_df = pd.concat([item[1] for item in train])\n",
    "valid_df = pd.concat([item[1] for item in valid])\n",
    "test_df = pd.concat([item[1] for item in test])\n",
    "\n",
    "# X-SAMPA tokenization\n",
    "def tokenize_x_sampa(transcription):\n",
    "    # List of known multi-character X-SAMPA symbols \n",
    "    # This list might need more symbols based on the entire dataset\n",
    "    multi_char_symbols = [\":l\", \"^m\", \"O:\"]\n",
    "    \n",
    "    tokens = []\n",
    "    for symbol in transcription.split():\n",
    "        if symbol in multi_char_symbols:\n",
    "            tokens.append(symbol)\n",
    "        else:\n",
    "            tokens.extend(list(symbol))\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the Transcription column for each dataframe\n",
    "train_df['tokenized_x_sampa'] = train_df['Transcription'].apply(tokenize_x_sampa)\n",
    "valid_df['tokenized_x_sampa'] = valid_df['Transcription'].apply(tokenize_x_sampa)\n",
    "test_df['tokenized_x_sampa'] = test_df['Transcription'].apply(tokenize_x_sampa)\n",
    "\n",
    "# Display the first few rows for verification\n",
    "print(train_df[['Transcription', 'tokenized_x_sampa']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Character tokenization the English text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (X-SAMPA symbols): ['\"', '.', '3', ':', '@', '^', 'a', 'b', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'z', 'รฐ', 'ล']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Function to convert the list of tokens back to space-separated string\n",
    "def tokens_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Convert the list of tokens in 'tokenized_x_sampa' back to space-separated string\n",
    "train_df['x_sampa_str'] = train_df['tokenized_x_sampa'].apply(tokens_to_string)\n",
    "\n",
    "# Initialize a CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer='word', token_pattern=r\"(?:\\S+)\")\n",
    "# We're using the token_pattern parameter to make sure we capture the entire X-SAMPA symbols\n",
    "\n",
    "# Apply the vectorizer on the 'x_sampa_str' column\n",
    "X = vectorizer.fit_transform(train_df['x_sampa_str'])\n",
    "\n",
    "# Getting the token names\n",
    "tokens = vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Tokens (X-SAMPA symbols):\", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tokenize the X-SAMPA transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Transcription                      tokenized_x_sampa\n",
      "34    fAr h@\"raIz@n   [f, A, r, h, @, \", r, a, I, z, @, n]\n",
      "35    fA: h@\"raIz@n   [f, A, :, h, @, \", r, a, I, z, @, n]\n",
      "92    Endl@s oUS@nz   [E, n, d, l, @, s, o, U, S, @, n, z]\n",
      "93    EndlIs oUS@nz   [E, n, d, l, I, s, o, U, S, @, n, z]\n",
      "98     glIs.nIN du:      [g, l, I, s, ., n, I, N, d, u, :]\n",
      "..              ...                                    ...\n",
      "153  wO:m \"s^nlaIt\"  [w, O:, m, \", s, ^, n, l, a, I, t, \"]\n",
      "70       raIzIN s^n            [r, a, I, z, I, N, s, ^, n]\n",
      "71       raIzIN s^n            [r, a, I, z, I, N, s, ^, n]\n",
      "172   sAft \"pIloUz\"   [s, A, f, t, \", p, I, l, o, U, z, \"]\n",
      "173   sOft \"pIl@Uz\"   [s, O, f, t, \", p, I, l, @, U, z, \"]\n",
      "\n",
      "[160 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# List of known multi-character X-SAMPA symbols (this is just a sample; there might be more)\n",
    "multi_char_symbols = [\":l\", \"^m\", \"O:\"]\n",
    "# If there are more multi-character symbols in your data, add them to the list\n",
    "\n",
    "def tokenize_x_sampa(transcription):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(transcription):\n",
    "        # Check if the current char and next char form a multi-char symbol\n",
    "        if i < len(transcription) - 1 and transcription[i:i+2] in multi_char_symbols:\n",
    "            tokens.append(transcription[i:i+2])\n",
    "            i += 2\n",
    "        # Check for single-char symbols\n",
    "        elif transcription[i] != ' ':\n",
    "            tokens.append(transcription[i])\n",
    "            i += 1\n",
    "        # If it's just a space, skip to the next character\n",
    "        else:\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "# Apply the function to the Transcription column of train_df\n",
    "train_df['tokenized_x_sampa'] = train_df['Transcription'].apply(tokenize_x_sampa)\n",
    "\n",
    "# Display the results for train_df\n",
    "print(train_df[['Transcription', 'tokenized_x_sampa']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Convert these tokens to integer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    english_str                                 english_seq  \\\n",
      "34        F a r   h o r i z o n         [19, 8, 5, 11, 10, 5, 2, 23, 10, 4]   \n",
      "35        F a r   h o r i z o n         [19, 8, 5, 11, 10, 5, 2, 23, 10, 4]   \n",
      "92  E n d l e s s   o c e a n s  [3, 4, 12, 6, 3, 1, 1, 10, 16, 3, 8, 4, 1]   \n",
      "93  E n d l e s s   o c e a n s  [3, 4, 12, 6, 3, 1, 1, 10, 16, 3, 8, 4, 1]   \n",
      "98  G l i s t e n i n g   d e w   [7, 6, 2, 1, 9, 3, 4, 2, 4, 7, 12, 3, 13]   \n",
      "\n",
      "                x_sampa_str                            x_sampa_seq  \n",
      "34  f A r h @ \" r a I z @ n         [21, 14, 5, 25, 5, 9, 1, 7, 3]  \n",
      "35  f A : h @ \" r a I z @ n            [21, 14, 25, 5, 9, 1, 7, 3]  \n",
      "92  E n d l @ s o U S @ n z     [26, 3, 8, 4, 2, 24, 12, 18, 3, 7]  \n",
      "93  E n d l I s o U S @ n z  [26, 3, 8, 4, 1, 2, 24, 12, 18, 3, 7]  \n",
      "98    g l I s . n I N d u :         [19, 4, 1, 2, 3, 1, 10, 8, 29]  \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Convert the list of tokens back to space-separated strings for both English and X-SAMPA\n",
    "train_df['english_str'] = train_df['Word'].apply(lambda x: ' '.join(list(x)))  # For English character tokenization\n",
    "train_df['x_sampa_str'] = train_df['tokenized_x_sampa'].apply(' '.join)\n",
    "\n",
    "valid_df['english_str'] = valid_df['Word'].apply(lambda x: ' '.join(list(x)))\n",
    "valid_df['x_sampa_str'] = valid_df['tokenized_x_sampa'].apply(' '.join)\n",
    "\n",
    "test_df['english_str'] = test_df['Word'].apply(lambda x: ' '.join(list(x)))\n",
    "test_df['x_sampa_str'] = test_df['tokenized_x_sampa'].apply(' '.join)\n",
    "\n",
    "# Initialize and fit the tokenizer for English text\n",
    "english_tokenizer = Tokenizer(char_level=False, lower=True, split=' ')\n",
    "english_tokenizer.fit_on_texts(train_df['english_str'])\n",
    "\n",
    "# Convert English tokens to integer sequences\n",
    "train_df['english_seq'] = english_tokenizer.texts_to_sequences(train_df['english_str'])\n",
    "valid_df['english_seq'] = english_tokenizer.texts_to_sequences(valid_df['english_str'])\n",
    "test_df['english_seq'] = english_tokenizer.texts_to_sequences(test_df['english_str'])\n",
    "\n",
    "# Initialize and fit the tokenizer for X-SAMPA transcriptions\n",
    "x_sampa_tokenizer = Tokenizer(char_level=False, lower=False, split=' ')\n",
    "x_sampa_tokenizer.fit_on_texts(train_df['x_sampa_str'])\n",
    "\n",
    "# Convert X-SAMPA tokens to integer sequences\n",
    "train_df['x_sampa_seq'] = x_sampa_tokenizer.texts_to_sequences(train_df['x_sampa_str'])\n",
    "valid_df['x_sampa_seq'] = x_sampa_tokenizer.texts_to_sequences(valid_df['x_sampa_str'])\n",
    "test_df['x_sampa_seq'] = x_sampa_tokenizer.texts_to_sequences(test_df['x_sampa_str'])\n",
    "\n",
    "# Let's check the conversion\n",
    "print(train_df[['english_str', 'english_seq', 'x_sampa_str', 'x_sampa_seq']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 3.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.1 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4.2 Split CSV file into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 5.1 Split CSV file into training, validation, and test sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
